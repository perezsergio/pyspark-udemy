{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is a general engine for big data processing.\n",
    "\n",
    "Big data tool -> Horizontal scaling, fault tolerance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark core + Std libraries (streaming, sql, mlib, graphx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance is achieved with a Directed Acyclic Graph engine (lazy engine).\n",
    "There's apis in python java and scala. Most common is python.\n",
    "\n",
    "A main concept of Spark is the Resilient Distributed Dataset (RDD) .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why python, not scala or java?\n",
    "\n",
    "  It's just easier to set up and to program. It's the most popular language, most people already know it.\n",
    "\n",
    "  An alternative is Scala. It is a tiny bit faster, and spark features are first come out on scala. This is because scala is written in sccala. However, python is better for most use cases.\n",
    "\n",
    "  Python and Scala code is very similar anyways.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why use spark instead of mapreduce?\n",
    "\n",
    "  It's 2-100 times faster, and it's easier to program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are a resilient (fault tolerant), distributed, dataset abstraction.\n",
    "\n",
    "In practice, you will create a RDD from some stored data, then you will transform it to get the information that you want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SparkContext** basically a spark context object gives you access to the methods that you need to create a RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation**\n",
    "From text file `sc.textfile` in local system or hdfs.\n",
    "From python object with `sc.parallelize`\n",
    "You can also create RDDs from hive objects with a HiveContext object.\n",
    "In general you can create an RDD from any popular big data formats (Cassandra, ElasticSearch, HBase) and normal data formats (json, csv, ...)\n",
    "\n",
    "**Transformation**\n",
    "The methods used to transform a rdd are: `map`, `flatmap`, `filter`, `distinct`, `sample`, and `union`, `intersection`, `subtract`, `cartesian`. As you can see, there aren't many methods, but they are all very powerful.\n",
    "\n",
    "**Actions**\n",
    "Common actions: collect, count, countByValue, take, top, reduce, ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has **lazy evaluation**, it won't do anything until there is an action call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key/value rdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key/value rdd is one of the most common and most useful patterns. They look like a python arr of 2d tuples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you map values only, and you don't modify the keys,\n",
    "it's more efficient to use `mapValues` and `flatMapValues`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
